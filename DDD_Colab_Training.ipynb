{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DDD Colab Training (Declip, 48k Mono)\n",
        "\n",
        "This notebook downloads your MEGA dataset, validates mono/48k audio, ",
        "creates train/valid/test splits, and trains the DDD model (HiFiGAN solver).\n",
        "Checkpoints and inference outputs are stored in Google Drive under the run name.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mount Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "RUN_NAME = \"ddd_48k_run1\"\n",
        "DRIVE_BASE_DIR = \"/content/drive/MyDrive/DeClip Data/runs\"\n",
        "REPO_URL = \"https://github.com/a-n-t-h-o-n-y/DDD.git\"\n",
        "REPO_DIR = \"/content/DDD\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "MEGA_LINK = \"https://mega.nz/file/6hsn1ajJ#n99Hn-6HZhEjMzPE6mzs1UdhxgpVd9XD83PWIxS6MXw\"\n",
        "DATA_ZIP_NAME = \"training_data.zip\"\n",
        "LOCAL_DATA_ROOT = \"/content/data_raw\"\n",
        "LOCAL_DATA_DIR = f\"{LOCAL_DATA_ROOT}/audio\"\n",
        "\n",
        "SEED = 1337\n",
        "TRAIN_RATIO = 0.90\n",
        "VALID_RATIO = 0.05\n",
        "TEST_RATIO = 0.05\n",
        "\n",
        "SAMPLE_RATE = 48000\n",
        "SAVE_EVERY = 10\n",
        "\n",
        "RUN_DIR = os.path.join(DRIVE_BASE_DIR, RUN_NAME)\n",
        "CHECKPOINT_DIR = os.path.join(RUN_DIR, \"checkpoints\")\n",
        "INFER_DIR = os.path.join(RUN_DIR, \"infer\")\n",
        "EGS_DIR = os.path.join(\"/content/egs\", RUN_NAME)\n",
        "\n",
        "Path(CHECKPOINT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "Path(INFER_DIR).mkdir(parents=True, exist_ok=True)\n",
        "Path(EGS_DIR).mkdir(parents=True, exist_ok=True)\n",
        "print(\"Run dir:\", RUN_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "subprocess.check_call([\"apt-get\", \"update\", \"-y\"])\n",
        "subprocess.check_call([\"apt-get\", \"install\", \"-y\", \"megatools\"])\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "subprocess.check_call([\"pip\", \"install\", \"-r\", \"requirements.txt\", \"-f\", \"https://download.pytorch.org/whl/torch_stable.html\", \"--no-cache-dir\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Dataset (MEGA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "zip_path = Path(LOCAL_DATA_ROOT) / DATA_ZIP_NAME\n",
        "Path(LOCAL_DATA_ROOT).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "if not zip_path.exists():\n",
        "    subprocess.check_call([\"megadl\", \"--path\", str(zip_path), MEGA_LINK])\n",
        "else:\n",
        "    print(\"Zip already exists:\", zip_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "Path(LOCAL_DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "    zf.extractall(LOCAL_DATA_DIR)\n",
        "print(\"Extracted to:\", LOCAL_DATA_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scan Wav Files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "wav_paths = sorted([str(p) for p in Path(LOCAL_DATA_DIR).rglob('*.wav')])\n",
        "print(f\"Found {len(wav_paths)} wav files\")\n",
        "if not wav_paths:\n",
        "    raise RuntimeError('No wav files found after extraction.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validate Audio (Mono, 48k)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchaudio\n",
        "\n",
        "bad = []\n",
        "for path in wav_paths:\n",
        "    info = torchaudio.info(path)\n",
        "    sr = info.sample_rate\n",
        "    ch = info.num_channels\n",
        "    issues = []\n",
        "    if ch != 1:\n",
        "        issues.append(f\"channels={ch}\")\n",
        "    if sr != SAMPLE_RATE:\n",
        "        issues.append(f\"sample_rate={sr}\")\n",
        "    if issues:\n",
        "        bad.append((path, \"; \".join(issues)))\n",
        "\n",
        "if bad:\n",
        "    print('Invalid files:')\n",
        "    for path, issues in bad:\n",
        "        print(f\"- {path}: {issues}\")\n",
        "    raise RuntimeError(f\"Validation failed for {len(bad)} file(s).\")\n",
        "\n",
        "print('All files are mono and 48kHz.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Train/Valid/Test Splits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "import torchaudio\n",
        "\n",
        "rng = random.Random(SEED)\n",
        "rng.shuffle(wav_paths)\n",
        "\n",
        "total = len(wav_paths)\n",
        "train_end = int(total * TRAIN_RATIO)\n",
        "valid_end = train_end + int(total * VALID_RATIO)\n",
        "\n",
        "train_paths = wav_paths[:train_end]\n",
        "valid_paths = wav_paths[train_end:valid_end]\n",
        "test_paths = wav_paths[valid_end:]\n",
        "\n",
        "def file_length(path):\n",
        "    info = torchaudio.info(path)\n",
        "    if hasattr(info, 'num_frames'):\n",
        "        return info.num_frames\n",
        "    siginfo = info[0]\n",
        "    return siginfo.length // siginfo.channels\n",
        "\n",
        "def write_split(split_name, clean_paths, noisy_paths=None):\n",
        "    split_dir = Path(EGS_DIR) / split_name\n",
        "    split_dir.mkdir(parents=True, exist_ok=True)\n",
        "    noisy_paths = noisy_paths if noisy_paths is not None else clean_paths\n",
        "    clean_meta = [(str(Path(p).resolve()), file_length(p)) for p in clean_paths]\n",
        "    noisy_meta = [(str(Path(p).resolve()), file_length(p)) for p in noisy_paths]\n",
        "    clean_path = split_dir / 'clean.json'\n",
        "    noisy_path = split_dir / 'noisy.json'\n",
        "    with open(clean_path, 'w') as f:\n",
        "        json.dump(clean_meta, f, indent=2)\n",
        "    with open(noisy_path, 'w') as f:\n",
        "        json.dump(noisy_meta, f, indent=2)\n",
        "    return str(split_dir)\n",
        "\n",
        "train_dir = write_split('tr', train_paths)\n",
        "valid_dir = write_split('cv', valid_paths)\n",
        "\n",
        "print(f\"Train: {len(train_paths)}, Valid: {len(valid_paths)}, Test: {len(test_paths)}\")\n",
        "print('EGS dirs:', train_dir, valid_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Clipped Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from pathlib import Path\n",
        "import torchaudio\n",
        "import torch\n",
        "\n",
        "TEST_NOISY_DIR = '/content/test_noisy'\n",
        "Path(TEST_NOISY_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "root_dir = Path(LOCAL_DATA_DIR).resolve()\n",
        "rng = random.Random(SEED)\n",
        "test_noisy_paths = []\n",
        "\n",
        "for src in test_paths:\n",
        "    src_path = Path(src).resolve()\n",
        "    rel = src_path.relative_to(root_dir)\n",
        "    dst_path = Path(TEST_NOISY_DIR) / rel\n",
        "    dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    wav, sr = torchaudio.load(str(src_path))\n",
        "    thres = 10 ** (rng.uniform(-2.0, -0.9))\n",
        "    wav = torch.clamp(wav, min=-thres, max=thres)\n",
        "    torchaudio.save(str(dst_path), wav, sr)\n",
        "    test_noisy_paths.append(str(dst_path))\n",
        "\n",
        "test_dir = write_split('ts', test_paths, test_noisy_paths)\n",
        "\n",
        "print('Clipped test set created at:', TEST_NOISY_DIR)\n",
        "print('EGS test dir:', test_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train DDD (HiFiGAN Solver)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "train_cmd = [\n",
        "    'python', 'train_hifigan.py',\n",
        "    '+augmentor=shift_only',\n",
        "    '+tr_loader=clippedclean',\n",
        "    '+cv_loader=clippedclean',\n",
        "    '+ts_loader=setting_1',\n",
        "    '+loss=setting_1',\n",
        "    '+model=setting_1',\n",
        "    '+optimizer=adamw_1e4',\n",
        "    '+experiment=setting_3_bs2',\n",
        "    '+solver=hifiganaudiotoaudio',\n",
        "    f'+data.train={train_dir}',\n",
        "    f'+data.valid={valid_dir}',\n",
        "    f'+data.test={test_dir}',\n",
        "    f'+experiment.output_path={CHECKPOINT_DIR}',\n",
        "    f'+experiment.save_every={SAVE_EVERY}',\n",
        "    f'+tr_loader.parameters.sample_rate={SAMPLE_RATE}',\n",
        "    f'+cv_loader.parameters.sample_rate={SAMPLE_RATE}',\n",
        "    f'+ts_loader.parameters.sample_rate={SAMPLE_RATE}',\n",
        "    f'+model.parameters.sample_rate={SAMPLE_RATE}',\n",
        "]\n",
        "\n",
        "subprocess.check_call(train_cmd)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Clipped Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "checkpoint_path = os.path.join(CHECKPOINT_DIR, 'checkpoint.th')\n",
        "\n",
        "infer_cmd = [\n",
        "    'python', 'infer_gan.py',\n",
        "    f'+load_from={checkpoint_path}',\n",
        "    f'+out_dir={INFER_DIR}',\n",
        "    '+model=setting_1',\n",
        "    '+experiment=setting_3',\n",
        "    f'+experiment.output_path={CHECKPOINT_DIR}',\n",
        "    '+experiment.device=cuda',\n",
        "    '+experiment.num_workers=2',\n",
        "    '+experiment.dry=0.0',\n",
        "    f'+noisy_dir={TEST_NOISY_DIR}',\n",
        "    f'+model.parameters.sample_rate={SAMPLE_RATE}',\n",
        "]\n",
        "\n",
        "subprocess.check_call(infer_cmd)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}